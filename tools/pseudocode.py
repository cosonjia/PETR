#########################################################################################################
###                               pseudocode of PETRv2 , PyTorch-like
#########################################################################################################

def transformation(l2e(t-1), e2g(t-1), l2e(t), e2g(t)):
    # Calculate the transformation matrix from lidar coordinates at time t to lidar coordinates at time t-1. 
    # l2e(t-1) is the transformation matrix from lidar coordinates to ego coordinates at time t - 1.
    # e2g(t-1) is the transformation matrix from ego coordinates to global coordinates at time t - 1.
    # l2e(t) is the transformation matrix from lidar coordinates to ego coordinates at time t.
    # e2g(t) is the transformation matrix from ego coordinates to global coordinates at time t.
    # The global coordinates is unique for different timestamps.

    T(t, t-1) = torch.inverse(l2e(t-1)) @ torch.inverse(e2g(t-1)) @ e2g(t) @ l2e(t) 
    # lidar(t) -> ego(t) -> global -> ego(t-1) -> lidar(t-1)

    return T(t, t-1)
    
def coordinates_alignment(x(t-1), l2img(t-1, t-1), x(t), l2img(t,t)): ##3D coordinates alignment module
    #Calculate the transformation from lidar coordinates at time t to history image coordinates t -1. 
    #x(t-1) is the image features at (t-1)
    #l2img(t-1, t-1) is the transformation matrix from lidar coordinates at (t-1) to image coordinates at (t-1).
    #x(t) is the image features at (t)
    #l2img(t, t) is the transformation matrix from lidar coordinates at (t) to image coordinates at (t).

    T(t, t-1) = transformation(l2e(t-1), e2g(t-1), l2e(t), e2g(t)) 
    l2img(t, t-1) = l2img(t-1, t-1) @ T(t, t-1) 
    #l2img(t, t-1) is the transformation matrix from lidar coordinates at (t) to image coordinates at (t-1).

    # The history frame (t âˆ’ 1) is concat with current frame (t) at view axis. 
    x = concat([x(t), x(t-1)]) #[(B,6,C,H,W), (B,6,C,H,W)] -> (B,12,C,H,W)
    l2img = concat([l2img(t,t), l2img(t, t-1)]) #[(B,6,4,4), (B,6,4,4)] -> (B,12,4,4)

    return x, l2img 

def fpe(x, l2img): #Feature guided module
    pe = Sigma(l2img, meshgrids)  
    #Sigma is the position embedding network in PETR, pe is generated by parameters and predefined grids.
    w = sigmoid(conv(x)) #Image features are used to generate weights.
    pe = w * pe #Weighting the 3D PE

    return pe

def main():

    x(t) = backbone(img(t)) #get img features at time t, x(t-1) can obtained from the cache of the last frame.
    x, l2img = coordinates_alignment(x(t-1), l2img(t-1, t-1), x(t), l2img(t,t)): ###do the coordinates_alignment
    pe = fpe(x, l2img)
    flat_x, flat_pe = flatten(x), flatten(pe) #flatten the tokens for transformer, (B,12,C,H,W) -> (B, C, 12*H*W)

    results = []
    quries = query_generator(anchor_points) #generate quries like PETR.
    for task in [det, seg, lane]:#process multi tasks separately.
        task_query = quries[task]
        task_query = transformer(task_query, flat_x, flat_pe)
        results.append(task_head(task_query))

    return results



